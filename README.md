# Speech to Note: Audio Transcription Tool

A powerful, GPU-accelerated audio transcription tool with Google Drive integration, built on Whisper AI and optimized for NVIDIA GTX 1080.

## ‚ú® Features

### Core Capabilities
- **Multi-format Support**: MP3, WAV, M4A, OGG, FLAC, AAC
- **Multiple Output Formats**: TXT, JSON, SRT, VTT subtitles
- **99+ Language Support** with automatic detection
- **GPU Acceleration** optimized for GTX 1080 (8GB VRAM)
- **Batch Processing** for multiple files
- **Google Drive Integration** for cloud storage access

### Advanced Features
- Word-level timestamps
- Voice Activity Detection (VAD)
- Real-time progress tracking
- Automatic format conversion
- Translation to English
- Web-based interface with Gradio

## üöÄ Quick Start

### Prerequisites
- Python 3.8+
- NVIDIA GTX 1080 or compatible GPU (optional, CPU fallback available)
- CUDA 11.8 (for GPU acceleration)
- FFmpeg
- Google Cloud API credentials (for Drive integration)

### Installation

#### Option 1: Automated Setup (Linux/macOS)
```bash
# Clone the repository
git clone https://github.com/yourusername/audio-transcription-tool.git
cd audio-transcription-tool

# Run setup script
chmod +x setup.sh
./setup.sh
```

#### Option 2: Manual Installation
```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install PyTorch with CUDA 11.8
pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0+cu118 --index-url https://download.pytorch.org/whl/cu118

# Install dependencies
pip install -r requirements.txt

# Create necessary directories
mkdir -p models downloads outputs
```

#### Option 3: Docker Deployment
```bash
# Build Docker image
docker build -t audio-transcription-tool .

# Run with GPU support
docker run --gpus all -p 7860:7860 -v $(pwd)/outputs:/app/outputs audio-transcription-tool
```

### Google Drive Setup

1. **Enable Google Drive API**:
   - Go to [Google Cloud Console](https://console.cloud.google.com/)
   - Create a new project or select existing
   - Enable Google Drive API
   - Create OAuth 2.0 credentials

2. **Configure Credentials**:
   - Download credentials as JSON
   - Rename to `client_secrets.json`
   - Place in application directory

3. **First-time Authentication**:
   - Click "Authenticate Google Drive" in the web interface
   - Follow the OAuth flow
   - Credentials will be saved for future use

## üìñ Usage

### Starting the Application
```bash
python3 audio_transcription_tool.py
```

Access the interface at: `http://localhost:7860`

### Web Interface

#### Single File Transcription
1. Upload audio file or provide Google Drive file ID
2. Select language (or use auto-detection)
3. Choose output format
4. Click "Start Transcription"
5. Download the result

#### Batch Processing
1. Upload multiple audio files
2. Select output format
3. Click "Start Batch Processing"
4. Download ZIP archive with all transcriptions

### Command Line Usage (Alternative)
```python
from audio_transcription_tool import TranscriptionEngine

# Initialize engine
engine = TranscriptionEngine(model_size="base", device="cuda")

# Transcribe file
result = engine.transcribe(
    "audio.mp3",
    language="auto",
    word_timestamps=True
)

# Save as SRT
from audio_transcription_tool import OutputFormatter
srt_content = OutputFormatter.to_srt(result)
with open("output.srt", "w") as f:
    f.write(srt_content)
```

## ‚öôÔ∏è Configuration

Edit `.env` file to customize settings:

```env
# Model Configuration
WHISPER_MODEL_SIZE=base  # tiny, base, small, medium, large
DEVICE=cuda              # cuda or cpu
COMPUTE_TYPE=float16     # float16, int8, float32

# Paths
MODEL_CACHE_DIR=./models
DOWNLOAD_DIR=./downloads
OUTPUT_DIR=./outputs

# Server Configuration
SERVER_HOST=0.0.0.0
SERVER_PORT=7860
SHARE_GRADIO=true
```

## üéØ Model Selection Guide

| Model | VRAM | Speed | Accuracy | Recommended For |
|-------|------|-------|----------|-----------------|
| tiny | 1GB | Fastest | Good | Quick drafts, real-time |
| base | 1GB | Fast | Better | **GTX 1080 optimal** |
| small | 2GB | Moderate | Great | GTX 1080 with quality focus |
| medium | 5GB | Slow | Excellent | High-end GPUs |
| large | 10GB | Slowest | Best | Professional use |

## üîß Troubleshooting

### CUDA Not Detected
```bash
# Check CUDA installation
nvidia-smi
nvcc --version

# Reinstall PyTorch with CUDA
pip uninstall torch torchvision torchaudio
pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0+cu118 --index-url https://download.pytorch.org/whl/cu118
```

### Out of Memory Error
- Reduce model size to "tiny" or "base"
- Enable INT8 quantization: `compute_type="int8"`
- Process shorter audio segments
- Close other GPU applications

### FFmpeg Issues
```bash
# Linux
sudo apt-get install ffmpeg

# macOS
brew install ffmpeg

# Windows
# Download from https://ffmpeg.org/download.html
```

### Google Drive Authentication Failed
1. Check `client_secrets.json` is valid
2. Delete `credentials.json` and re-authenticate
3. Ensure redirect URI matches: `http://localhost:8080/`

## üìä Performance Benchmarks

### GTX 1080 (8GB VRAM) Performance

| Model | Audio Length | Processing Time | Memory Usage |
|-------|--------------|-----------------|--------------|
| base | 1 hour | ~2 minutes | 2.5GB |
| base | 10 minutes | ~20 seconds | 2.5GB |
| small | 1 hour | ~4 minutes | 3.5GB |
| small | 10 minutes | ~40 seconds | 3.5GB |

*Using faster-whisper with float16 precision and VAD filter*

## üõ†Ô∏è Advanced Usage

### Custom Processing Pipeline
```python
# Custom language model
engine = TranscriptionEngine(
    model_size="small",
    device="cuda",
    compute_type="int8"  # For memory optimization
)

# Process with specific settings
result = engine.transcribe(
    "audio.mp3",
    language="en",  # Force English
    task="translate",  # Translate to English
    word_timestamps=True,
    vad_filter=True,
    vad_parameters={
        "threshold": 0.5,
        "min_speech_duration_ms": 250,
        "min_silence_duration_ms": 2000
    }
)
```

### Batch Processing with Progress
```python
def process_folder(folder_path):
    files = Path(folder_path).glob("*.mp3")
    
    for file in files:
        print(f"Processing: {file.name}")
        result = engine.transcribe(str(file))
        
        # Save transcript
        output_path = file.with_suffix(".txt")
        with open(output_path, "w") as f:
            f.write(result["text"])
```

## üìù API Reference

### TranscriptionEngine

```python
class TranscriptionEngine:
    def __init__(self, model_size="base", device="cuda", compute_type="float16")
    def transcribe(self, audio_path, language=None, task="transcribe", 
                  word_timestamps=False, progress_callback=None)
```

### GoogleDriveManager

```python
class GoogleDriveManager:
    def authenticate(self)
    def list_audio_files(self, folder_id=None)
    def download_file(self, file_id, destination)
```

### OutputFormatter

```python
class OutputFormatter:
    @staticmethod
    def to_text(result)
    def to_json(result)
    def to_srt(result)
    def to_vtt(result)
```

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- [OpenAI Whisper](https://github.com/openai/whisper) for the amazing ASR model
- [faster-whisper](https://github.com/guillaumekln/faster-whisper) for optimized inference
- [Gradio](https://gradio.app/) for the web interface
- [PyDrive2](https://github.com/iterative/PyDrive2) for Google Drive integration

## üìß Support

For issues and questions:
- Open an [Issue](https://github.com/yourusername/audio-transcription-tool/issues)
- Check [FAQ](docs/FAQ.md)
- Contact: your.email@example.com

---

**Note**: This tool is optimized for NVIDIA GTX 1080 but works with any CUDA-capable GPU or CPU. Adjust model size based on your hardware capabilities.